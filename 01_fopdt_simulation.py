# -*- coding: utf-8 -*-
"""01_FOPDT_Simulation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AN-C3aaXxrGatg8IEXu16Z2MH-cIAjz5

## 1. FOPDT Simulation for Generating training Data
"""

# try:
#     from gekko import GEKKO
# except:
#     pip install gekko
#     # restart kernel if this doesn't import
#     from gekko import GEKKO

from gekko import GEKKO
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# from smt.sampling_methods import LHS
# from smt.sampling_methods import Random

tf = 30 # fianl time

m=GEKKO(remote=False)
m.time = np.linspace(0,tf-1,tf)

K = m.FV(1) # Process Gain
tau = m.FV(2) # Time Constant

u_input = np.zeros(tf)
u_input[5:] = 1

y = m.CV()
u = m.MV(u_input)
m.Equation(tau*y.dt()+y==K*u) #FOPDT Equation

m.options.IMODE=4

m.solve(disp=False)

plt.figure(0)
plt.subplot(2,1,1)
plt.plot(m.time, y)
plt.subplot(2,1,2)
plt.plot(m.time, u, drawstyle='steps')
plt.show()

"""## 2. Generating input signal (Random and LHS)"""

import numpy as np
import matplotlib.pyplot as plt

nstep = 800 # Choose training data lenth

# random signal generation

a_range = [0,2]
a = np.random.rand(nstep) * (a_range[1]-a_range[0]) + a_range[0] # range for amplitude
a[0] = 0

b_range = [5, 20]
b = np.random.rand(nstep) *(b_range[1]-b_range[0]) + b_range[0] # range for frequency
b = np.round(b)
b = b.astype(int)

b[0] = 0

for i in range(1,np.size(b)):
    b[i] = b[i-1]+b[i]

# Random Signal
i=0
random_signal = np.zeros(nstep)
while b[i]<np.size(random_signal):
    k = b[i]
    random_signal[k:] = a[i]
    i=i+1

# PRBS
a = np.zeros(nstep)
j = 0
while j < nstep:
    a[j] = 5
    a[j+1] = -5
    j = j+2

i=0
prbs = np.zeros(nstep)
while b[i]<np.size(prbs):
    k = b[i]
    prbs[k:] = a[i]
    i=i+1

plt.figure(0) 
plt.subplot(2,1,1)
plt.plot(random_signal, drawstyle='steps',label='Random Signal')
plt.legend()
plt.subplot(2,1,2)
plt.plot(prbs, drawstyle='steps', label='PRBS')
plt.legend()
plt.show()

"""## 3. Evaluate the Random Input Signal"""

tf = nstep
m.time = np.linspace(0,tf-1,tf)
u.value = random_signal

m.options.IMODE = 4
m.solve(disp=False)

plt.figure(1)
plt.subplot(2,1,1)
plt.plot(m.time, y)
plt.subplot(2,1,2)
plt.plot(m.time, u, drawstyle='steps')
plt.show()

"""## 4. LSTM fitting"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import joblib
import time

# For LSTM model
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
from keras.callbacks import EarlyStopping
from keras.models import load_model

window = 5
#Load training data
data = pd.DataFrame(
        {"u": u,
         "y": y},
        index = np.linspace(1,nstep,nstep,dtype=int))

# Scale features
s1 = MinMaxScaler(feature_range=(-1,1))
Xs = s1.fit_transform(data[['u','y']])
s2 = MinMaxScaler(feature_range=(-1,1))
Ys = s2.fit_transform(data[['y']])

Ys.shape

Xs.shape

plt.plot(Ys)
plt.plot(Xs)

joblib.dump(s1, 's1.sav')
joblib.dump(s2, 's2.sav')

val_ratio = 0.5
cut_index = np.int(nstep*val_ratio)
print(cut_index)
Xs_train = Xs[0:cut_index]
Ys_train = Ys[0:cut_index]
Xs_val = Xs[cut_index:]
Ys_val = Ys[cut_index:]

X_train = []
Y_train = []
for i in range(window,len(Xs_train)):
    X_train.append(Xs_train[i-window:i,:])
    Y_train.append(Ys_train[i])

X_val = []
Y_val = []
for i in range(window,len(Xs_val)):
    X_val.append(Xs_val[i-window:i,:])
    Y_val.append(Ys_val[i])

# Reshape data to format accepted by LSTM
X_train, Y_train = np.array(X_train), np.array(Y_train)
X_val, Y_val = np.array(X_val), np.array(Y_val)

np.shape(X_train), np.shape(Y_train)

# # Initialize LSTM model
model = Sequential()

model.add(LSTM(units=100, return_sequences=True, \
          input_shape=(X_train.shape[1],X_train.shape[2])))
model.add(Dropout(0.2))
model.add(LSTM(units=100, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=100))
model.add(Dropout(0.2))
model.add(Dense(units=1)) #units = number of outputs
model.compile(optimizer = 'adam', loss = 'mean_squared_error',\
              metrics = ['accuracy'])
# Allow for early exit
es = EarlyStopping(monitor='loss',mode='min',verbose=1,patience=10)

# Fit (and time) LSTM model
t0 = time.time()
history = model.fit(X_train, Y_train, epochs = 300, batch_size = 250, callbacks=[es], verbose=1, validation_data=(X_val, Y_val))
t1 = time.time()
print('Runtime: %.2f s' %(t1-t0))

# Plot loss
plt.figure(4, figsize=(8,4))
plt.semilogy(history.history['loss'],label='train_loss')
plt.semilogy(history.history['val_loss'],label='val_loss')
plt.xlabel('epoch'); plt.ylabel('loss')
plt.savefig('FOPDT_loss.png')
plt.legend()
model.save('model.h5')
plt.show()

Yp_train = model.predict(X_train)

np.shape(Yp_train)

# Verify the fit of the model
Yp_train = model.predict(X_train)
Yp_val = model.predict(X_val)

# un-scale outputs
Yu_train = s2.inverse_transform(Yp_train)
Ym_train = s2.inverse_transform(Y_train)

Yu_val = s2.inverse_transform(Yp_val)
Ym_val = s2.inverse_transform(Y_val)

plt.figure(5, figsize=(12,2.5))
plt.subplot(1,2,1)
plt.plot(data.index[window:cut_index],Yu_train[:,0],'r-',label='LSTM')
plt.plot(data.index[window:cut_index],Ym_train[:,0],'b--',label='Measured')
plt.title('Training')
plt.legend()
plt.subplot(1,2,2)
plt.plot(data.index[cut_index+window:],Yu_val[:,0],'r-',label='LSTM')
plt.plot(data.index[cut_index+window:],Ym_val[:,0],'b--',label='Measured')
plt.title('Validation')
plt.legend()
plt.show()

